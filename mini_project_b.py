# -*- coding: utf-8 -*-
"""Mini Project B

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFH2AyZkWj69YGzj86AULCfF7wc67NEF
"""

!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers numpy pandas beautifulsoup4 requests
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import requests
from bs4 import BeautifulSoup
import re

tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

tokens = tokenizer.encode('This bill can be usefull to students who prosper but it is bad  to students who dont a good financial background', return_tensors='pt')
result = model(tokens)

sentiment=['Very Opposing','Slightly Negative','Needs improvement','Slightly positive','Complete Grant']
print(result.logits)
sentiment[int(torch.argmax(result.logits))]

model

import requests
from bs4 import BeautifulSoup
url='https://prsindia.org/billtrack/the-jammu-and-kashmir-local-bodies-laws-amendment-bill-2024'
html=requests.get(url)
s=BeautifulSoup(html.content,'html.parser')
# print(s.prettify())
key=s.find_all('a',title='PRS Bill Summary')
for i in key:
  k=i.get('href')
key
url2=('https://prsindia.org'+k)
url2
html2=requests.get(url2)
s1=BeautifulSoup(html2.content,'html.parser')
key1=s1.find(class_='body_content')

#TO GET HEADING AND SUMMARY OF BILL
import unicodedata
key_points=[]
heading=[]
ministry=[]
status=[]
pdf_link=[]
k=s.find_all('ul')

if not k:
  k=s.find_all('ul')
  for i in range(3,len(k)-5):
     key_points.append(unicodedata.normalize("NFKD",k[i].get_text(strip='True')))
  #  print(k[i].get_text(strip='True')+'\n')
elif len(k)==1:
  k=key1.find_all('span',style='font-size:16px')
  for i in k:
    key_points.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
else:
  for i in k:
   key_points.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
head=s.find_all('a',class_='active fs-28')
for i in head:
  heading.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
mini=s.find('div',class_='field field-name-field-ministry field-type-taxonomy-term-reference field-label-inline clearfix')
for i in mini:
  ministry.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
stat=s.find_all('div',class_='field field-name-field-own-status field-type-list-text field-label-hidden')
for i in stat:
   status.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))

print(key1.prettify())

print('\n HEADING : \n\n'+heading[0])
print('\n KEY POINTS : \n')
for i in range(len(key_points)-1):
  print(key_points[i]+'\n')
print('\n MINISTRY : \n\n'+ministry[3]+'\n\n')
print(' STATUS : \n')
if status[len(status)-1]=='Passed':
  print('Passed')
else:
  print('Pending')

"""**COMPILED CLASS BELOW FOR SCRAPING**"""

import requests
from bs4 import BeautifulSoup
import unicodedata
class bill_info:
  def __init__(self,url):
    self.url = url

  def scrape(self):
    key_points=[]
    heading=[]
    ministry=[]
    status=[]
    data=[]
    html=requests.get(self.url)
    s=BeautifulSoup(html.content,'html.parser')
    key=s.find_all('a',title='PRS Bill Summary')
    for i in key:
      k=i.get('href')
    url2=('https://prsindia.org'+k)
    url2
    html2=requests.get(url2)
    s1=BeautifulSoup(html2.content,'html.parser')
    key1=s1.find(class_='body_content')
    k=key1.find_all('ul')
    if not k:
     k=s.find_all('ul')
     for i in range(3,len(k)-5):
        key_points.append(unicodedata.normalize("NFKD",k[i].get_text(strip='True')))
    elif len(k)==1:
      k=key1.find_all('span',style='font-size:16px')
      for i in k:
        key_points.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
    else:
      for i in k:
        key_points.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
    head=s.find_all('a',class_='active fs-28')
    for i in head:
      heading.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
    mini=s.find('div',class_='field field-name-field-ministry field-type-taxonomy-term-reference field-label-inline clearfix')
    for i in mini:
      ministry.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
    stat=s.find_all('div',class_='field field-name-field-own-status field-type-list-text field-label-hidden')
    for i in stat:
      status.append(unicodedata.normalize("NFKD",i.get_text(strip='True')))
    data=[heading,ministry,status,key_points]
    return data


  def show(self):
    print('\n HEADING : \n\n'+heading[0])
    print('\n KEY POINTS : \n')
    global text
    text=''
    for i in range(len(key_points)-1):
      text+=key_points[i]
      print(key_points[i]+'\n')
    print('\n MINISTRY : \n\n'+ministry[3]+'\n\n')
    print(' STATUS : \n')
    if status[len(status)-1]=='Passed':
      print('Passed')
    else:
     print('Pending')

url='https://prsindia.org/billtrack/the-jammu-and-kashmir-local-bodies-laws-amendment-bill-2024'
bill=bill_info(url)
data=bill.scrape()
bill.show()

print(text)

f=text.split()
max1=int(len(f)*0.7)
min1=int(len(f)*0.5)
max1=max1//2
min1=min1//2

from transformers import pipeline

summarizer = pipeline("summarization", model="Falconsai/text_summarization")
out=summarizer(text, max_length=max1, min_length=min1, do_sample=False)
print(out[0]['summary_text'])

fin=out[0]['summary_text']
x=fin.split(".")
for sen in x:
  print(sen)

from transformers import PegasusForConditionalGeneration, PegasusTokenizer

# Load the Pegasus model and tokenizer
model_name = "google/pegasus-cnn_dailymail"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate the summary
summary_ids = model.generate(inputs["input_ids"], max_length=max1, min_length=min1, num_beams=4, early_stopping=False)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

fin=summary.split("<n>")
for s in fin:
  print(s)

!pip install googletrans==4.0.0-rc1

from googletrans import Translator

# Initialize the translator
translator = Translator()

def translate(text: str, dest_language: str) -> str:
    # Translate the text
    translation = translator.translate(text, dest=dest_language)
    return translation.text

# Example usage
text =summary

translated_text_hi = translate(text, 'hi')  # Hindi
translated_text_kn = translate(text, 'kn')  # Kannada
translated_text_te = translate(text, 'te')  # Telugu

print("Translated to Hindi:", translated_text_hi)
print("Translated to Kannada:", translated_text_kn)
print("Translated to Telugu:", translated_text_te)

