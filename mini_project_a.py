# -*- coding: utf-8 -*-
"""Mini Project A

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z814vD3bQjhMpRru3SZDA2Ru11OTKqiv
"""

!pip install tensorflow torch

import pandas as pd
import sqlite3
from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Connect to SQLite database
conn = sqlite3.connect('constitution.db')

# Load the articles DataFrame from the SQLite database
df_articles = pd.read_sql_query("SELECT * FROM articles", conn)

# Close the connection
conn.close()

# Initialize the BART model and tokenizer
model_name = "facebook/bart-large-cnn"
model = BartForConditionalGeneration.from_pretrained(model_name)
tokenizer = BartTokenizer.from_pretrained(model_name)

def search_article(keyword=None, article_number=None):
    if keyword:
        result = df_articles[df_articles['article_desc'].str.contains(keyword, case=False)]
    elif article_number:
        result = df_articles[df_articles['article_id'] == article_number]
    else:
        result = pd.DataFrame()
    return result

def generate_summary(text, level):
    # Define the number of words for each level
    lengths = {1: 60, 2: 120, 3: 180, 4: 240, 5: 350}
    max_length = lengths.get(level, 120)

    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, max_length=max_length, min_length=max_length//2, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def main():
    print("Welcome to the Constitution Article Summarizer!")
    choice = input("Search by (1) Keyword or (2) Article Number? ")

    if choice == '1':
        keyword = input("Enter the keyword: ")
        articles = search_article(keyword=keyword)
    elif choice == '2':
        article_number = input("Enter the article number (e.g., 'Article 5' or 'Article 52A'): ").strip()
        articles = search_article(article_number=article_number)
    else:
        print("Invalid choice!")
        return

    if articles.empty:
        print("No articles found.")
        return

    print(f"Found {len(articles)} articles.")
    for idx, row in articles.iterrows():
        print(f"\nArticle ID: {row['article_id']}")
        print(f"Description: {row['article_desc'][:100]}...")  # Print the first 100 characters

    article_id = input("\nEnter the article ID to summarize (e.g., 'Article 5'): ").strip()
    article = articles[articles['article_id'] == article_id]

    if article.empty:
        print("Invalid article ID.")
        return

    level = int(input("Enter summary level (1-5): "))
    if level < 1 or level > 5:
        print("Invalid summary level.")
        return

    text = article.iloc[0]['article_desc']
    summary = generate_summary(text, level)
    print("\nSummary:")
    print(summary)

if __name__ == "__main__":
    main()

# Install required libraries
!pip install transformers torch pandas textstat nltk

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Lowercase the text
    text = text.lower()

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize the text
    words = word_tokenize(text)

    # Remove stopwords and lemmatize
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]

    # Join words back into a single string
    processed_text = ' '.join(words)

    return processed_text

# Example usage
example_text = """
Article 5: Citizenship at the commencement of the Constitution.
At the commencement of this Constitution, every person who has his domicile in the territory of India and â€“
(a) who was born in the territory of India; or
(b) either of whose parents was born in the territory of India; or
(c) who has been ordinarily resident in the territory of India for not less than five years immediately preceding such commencement, shall be a citizen of India.
"""

processed_example = preprocess_text(example_text)
print(processed_example)

import pandas as pd
import sqlite3
from transformers import BartForConditionalGeneration, BartTokenizer
import torch
import textstat
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
from transformers import pipeline
import textstat
nltk.download('punkt')
nltk.download('stopwords')

# Connect to SQLite database
conn = sqlite3.connect('constitution.db')

# Load the articles DataFrame from the SQLite database
df_articles = pd.read_sql_query("SELECT * FROM articles", conn)

# Close the connection
conn.close()

# Display the first few rows to ensure data is loaded correctly
df_articles.head()

# Initialize the mBART model and tokenizer
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, pipeline
model_name = "facebook/mbart-large-50-many-to-one-mmt"
model = MBartForConditionalGeneration.from_pretrained(model_name)
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
from transformers import PegasusForConditionalGeneration, PegasusTokenizer
import torch

model_name = "google/pegasus-xsum"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)

def search_article(keyword=None, article_number=None):
    if keyword:
        result = df_articles[df_articles['article_desc'].str.contains(keyword, case=False)]
    elif article_number:
        result = df_articles[df_articles['article_id'] == article_number]
    else:
        result = pd.DataFrame()
    return result

def simplify_text(text):
    return text

# Function to generate summary
def generate_summary_falc(text, level):

    f=text.split()
    max1=int(len(f)*level*0.2)
    min1=int(0.2*len(f))

    summarizer = pipeline("summarization", model="Falconsai/text_summarization")
    out=summarizer(text, max_length=max1, min_length=min1, do_sample=False)
    simplified_summary=out[0]['summary_text']

    return simplified_summary

from transformers import PegasusForConditionalGeneration, PegasusTokenizer

# Load the Pegasus model and tokenizer
model_name = "google/pegasus-cnn_dailymail"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

def generate_summary_peg(text, level):
    # Define the number of tokens for each level
    lengths = {1: 60, 2: 120, 3: 180, 4: 240, 5: 300}
    max_new_tokens = lengths.get(level, 120)

    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate the summary
    summary_ids = model.generate(inputs["input_ids"], max_length=max_new_tokens, min_length=int(max_new_tokens * 0.5), num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    return summary

def main():
    print("Welcome to the Constitution Article Summarizer!")
    choice = input("Search by (1) Keyword or (2) Article Number? ")

    if choice == '1':
        keyword = input("Enter the keyword: ")
        articles = search_article(keyword=keyword)
    elif choice == '2':
        article_number = input("Enter the article number (e.g., 'Article 5' or 'Article 52A'): ").strip()
        articles = search_article(article_number=article_number)
    else:
        print("Invalid choice!")
        return

    if articles.empty:
        print("No articles found.")
        return

    print(f"Found {len(articles)} articles.")
    for idx, row in articles.iterrows():
        print(f"\nArticle ID: {row['article_id']}")
        print(f"Description: {row['article_desc'][:100]}...")  # Print the first 100 characters

    article_id = input("\nEnter the article ID to summarize (e.g., 'Article 5'): ").strip()
    article = articles[articles['article_id'] == article_id]

    if article.empty:
        print("Invalid article ID.")
        return

    level = int(input("Enter summary level (1-5): "))
    if level < 1 or level > 5:
        print("Invalid summary level.")
        return

    text = article.iloc[0]['article_desc']
    global sum1
    global sum2
    summary = generate_summary_falc(text, level)
    sum1=summary
    print("\nSummary from Extractive Model:")
    print(summary)

    summary = generate_summary_peg(text, level)
    sum2=summary
    print("\nSummary from Abstractive Model:")
    print(summary)


if __name__ == "__main__":
    main()

new=sum2.split("<n>")
for sentence in new:
  print(sentence)

!pip install googletrans==4.0.0-rc1

from googletrans import Translator

# Initialize the translator
translator = Translator()

def translate(text: str, dest_language: str) -> str:
    # Translate the text
    translation = translator.translate(text, dest=dest_language)
    return translation.text

# Example usage
text =sum2

translated_text_hi = translate(text, 'hi')  # Hindi
translated_text_kn = translate(text, 'kn')  # Kannada
translated_text_te = translate(text, 'te')  # Telugu

print("Translated to Hindi:", translated_text_hi)
print("Translated to Kannada:", translated_text_kn)
print("Translated to Telugu:", translated_text_te)

